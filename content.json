{"pages":[],"posts":[{"title":"TPU 한달 무제한 사용기 - TFRC (1)","text":"왜 TPU를 해야하는가?요즘 논문들을 읽다보면 많은 분야에서 모델의 크기가 계속해서 커지고 있습니다. Transformer 이후로 NLP 모델들은 계속 커져가고 GPT-3의 경우는 개인이 학습시키는 일은 불가능하며 이미지 모델들도 Transformer 영향을 받은 DeTR 외에도 점점 GPU 요구량이 커지고 있습니다. 그래서 저는 돈없는 학생으로서 살아남기위해 엄청난 메모리와 계산력을 가진 TPU를 공부하기로 마음먹었습니다. Multilingual Model 대회에 참가한 적이 있는데, XLM-Roberta Large 학습에서 GTX-1080Ti로는 3일 소요되지만 TPU-v2로는 1시간 걸리는 걸 보고 TPU가 추후에 많이 활용될 거 같다고 느꼈습니다. TPU 성능 그림 1. TPU 성능 비교표. Task에 따라 달라질 수 있지만 이 포스트에서는 자세히 다루지 않을 예정이다. TPU 사용 방법TPU 사용할 수 있는 방법은 현재 3가지가 있습니다. Colab Kaggle Notebook GCP를 통한 TPU API 활용 Colab과 Kaggle Notebook의 경우는 무료로 제공하기 때문에 메모리와 시간제약이 있습니다. Colab Pro의 경우도 TPU를 하루종일 사용하면 한동안 사용이 불가능합니다. Kaggle Notebook은 16GB 메모리 제한과 1회 3시간 이용가능한 제한이 있습니다. 그래서 가장 자유롭게 사용하는 방법은 GCP를 통해 TPU를 사용하는 방법입니다. 위 두 방법과는 다르게 유료이지만 연구자라면 한달동안 TFRC를 통해 5 * TPU-v2, 5 * TPU-v3, 조건적인 100 * TPU-v2. 최대 100개 이상의 TPU를 한달간 무제한으로 사용할 수 있습니다. TFRC 신청TFRC는 Tensorflow Research Cloud 웹페이지에서 신청할수 있습니다. ( https://www.tensorflow.org/tfrc?hl=ko ) TFRC 웹페이지에서 ‘지금 적용하기’ 클릭 Google Form에 간단한 정보 작성 (이름, 소속, 이메일, …) GCP 계정과 연결 위의 google form을 보내면, 적었던 메일로 확인 메일이 하나 온다. 여기서 GCP 계정정보를 적어서 전달합니다. TPU API 한달 무료이용 등록완료 위 메일이 날라오면 이제부터 31일간 TPU를 무료로 이용할 수 있습니다. 이 후 포스팅에서 GCP bucket과 VM을 통해 TPU 환경을 세팅 및 실제 이미지 데이터를 학습하는 과정을 적어보도록 하겠습니다.","link":"/2020/07/04/TPU-%ED%95%9C%EB%8B%AC-%EB%AC%B4%EC%A0%9C%ED%95%9C-%EC%82%AC%EC%9A%A9%EA%B8%B0-TFRC-1/"}],"tags":[{"name":"Tutorial","slug":"Tutorial","link":"/tags/Tutorial/"}],"categories":[{"name":"TPU","slug":"TPU","link":"/categories/TPU/"}]}